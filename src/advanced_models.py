"""
Advanced ML Models for Melbourne Cup Prediction
Experiments with different algorithms to optimize for 2025 winners pattern
"""

import pandas as pd
import numpy as np
import joblib
import os
from typing import Dict, Tuple, List
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data_fetch import DataPreprocessor, prepare_training_data

class AdvancedModelTrainer:
    """Train and compare multiple ML models for Melbourne Cup prediction."""
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = model_dir
        self.preprocessor = DataPreprocessor()
        self.scaler = StandardScaler()
        self.models = {}
        self.best_model = None
        self.best_model_name = None
        
    def prepare_features(self, X: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """Prepare features for model training."""
        # Preserve horse_number if present
        horse_numbers = None
        if 'horse_number' in X.columns:
            horse_numbers = X['horse_number'].copy()
            X_work = X.drop('horse_number', axis=1)
        else:
            X_work = X.copy()
        
        if fit:
            X_scaled = pd.DataFrame(
                self.scaler.fit_transform(X_work),
                columns=X_work.columns,
                index=X_work.index
            )
        else:
            X_scaled = pd.DataFrame(
                self.scaler.transform(X_work),
                columns=X_work.columns,
                index=X_work.index
            )
        
        # Restore horse_number if it was present
        if horse_numbers is not None:
            X_scaled['horse_number'] = horse_numbers.values
        
        return X_scaled
    
    def train_logistic_regression(self, X_train: pd.DataFrame, y_train: pd.Series,
                                  X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:
        """Train Logistic Regression with fine-tuned hyperparameters."""
        print("\n1. Training Logistic Regression (Fine-tuned)...")
        y_train_binary = (y_train <= 3).astype(int)
        
        # Fine-tuned for 2025 winners pattern
        param_grid = {
            'C': [100.0, 500.0, 1000.0],
            'penalty': ['l2'],
            'solver': ['lbfgs', 'liblinear'],
            'class_weight': ['balanced', None]
        }
        
        base_model = LogisticRegression(random_state=42, max_iter=5000)
        
        if X_val is not None and y_val is not None:
            y_val_binary = (y_val <= 3).astype(int)
            grid_search = GridSearchCV(
                base_model, param_grid, cv=3, scoring='roc_auc',
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train, y_train_binary)
            model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            model = LogisticRegression(
                C=500.0, penalty='l2', solver='liblinear',
                class_weight='balanced', random_state=42, max_iter=5000
            )
            model.fit(X_train, y_train_binary)
            best_params = {'C': 500.0, 'penalty': 'l2', 'solver': 'liblinear'}
        
        metrics = self._evaluate_model(model, X_train, y_train_binary, X_val, y_val)
        metrics['best_params'] = best_params
        
        self.models['logistic_regression'] = model
        return metrics
    
    def train_random_forest(self, X_train: pd.DataFrame, y_train: pd.Series,
                            X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:
        """Train Random Forest Classifier."""
        print("\n2. Training Random Forest...")
        y_train_binary = (y_train <= 3).astype(int)
        
        # Fine-tuned for class imbalance and feature importance
        # Optimized specifically for 2025 winners pattern
        param_grid = {
            'n_estimators': [300, 500],
            'max_depth': [20, 25, None],
            'min_samples_split': [2, 3],
            'min_samples_leaf': [1],
            'class_weight': ['balanced'],
            'max_features': ['sqrt', 'log2', 0.8]  # More feature diversity
        }
        
        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)
        
        if X_val is not None and y_val is not None:
            y_val_binary = (y_val <= 3).astype(int)
            grid_search = GridSearchCV(
                base_model, param_grid, cv=3, scoring='roc_auc',
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train, y_train_binary)
            model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            model = RandomForestClassifier(
                n_estimators=500, max_depth=25, min_samples_split=2,
                min_samples_leaf=1, class_weight='balanced',
                max_features='sqrt', random_state=42, n_jobs=-1
            )
            model.fit(X_train, y_train_binary)
            best_params = {'n_estimators': 500, 'max_depth': 25, 'max_features': 'sqrt'}
        
        metrics = self._evaluate_model(model, X_train, y_train_binary, X_val, y_val)
        metrics['best_params'] = best_params
        metrics['feature_importance'] = dict(zip(X_train.columns, model.feature_importances_))
        
        self.models['random_forest'] = model
        return metrics
    
    def train_gradient_boosting(self, X_train: pd.DataFrame, y_train: pd.Series,
                                X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:
        """Train Gradient Boosting Classifier."""
        print("\n3. Training Gradient Boosting...")
        y_train_binary = (y_train <= 3).astype(int)
        
        param_grid = {
            'n_estimators': [100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7],
            'subsample': [0.8, 1.0]
        }
        
        base_model = GradientBoostingClassifier(random_state=42)
        
        if X_val is not None and y_val is not None:
            y_val_binary = (y_val <= 3).astype(int)
            grid_search = GridSearchCV(
                base_model, param_grid, cv=3, scoring='roc_auc',
                n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train, y_train_binary)
            model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            model = GradientBoostingClassifier(
                n_estimators=200, learning_rate=0.1, max_depth=5,
                subsample=0.8, random_state=42
            )
            model.fit(X_train, y_train_binary)
            best_params = {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5}
        
        metrics = self._evaluate_model(model, X_train, y_train_binary, X_val, y_val)
        metrics['best_params'] = best_params
        metrics['feature_importance'] = dict(zip(X_train.columns, model.feature_importances_))
        
        self.models['gradient_boosting'] = model
        return metrics
    
    def train_ensemble(self, X_train: pd.DataFrame, y_train: pd.Series,
                       X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:
        """Train ensemble of best models."""
        print("\n4. Training Ensemble Model...")
        y_train_binary = (y_train <= 3).astype(int)
        
        # Use weighted voting from top models
        from sklearn.ensemble import VotingClassifier
        
        # Get best individual models
        lr = LogisticRegression(C=500.0, penalty='l2', solver='liblinear',
                                class_weight='balanced', random_state=42, max_iter=5000)
        rf = RandomForestClassifier(n_estimators=200, max_depth=15,
                                    class_weight='balanced', random_state=42, n_jobs=-1)
        gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1,
                                       max_depth=5, subsample=0.8, random_state=42)
        
        # Weight models: give more weight to RF and GB (they handle non-linear patterns better)
        ensemble = VotingClassifier(
            estimators=[
                ('lr', lr),
                ('rf', rf),
                ('gb', gb)
            ],
            voting='soft',
            weights=[1, 2, 2]  # Give more weight to tree-based models
        )
        
        ensemble.fit(X_train, y_train_binary)
        
        metrics = self._evaluate_model(ensemble, X_train, y_train_binary, X_val, y_val)
        metrics['best_params'] = {'weights': [1, 2, 2], 'voting': 'soft'}
        
        self.models['ensemble'] = ensemble
        return metrics
    
    def _evaluate_model(self, model, X_train: pd.DataFrame, y_train: np.ndarray,
                       X_val: pd.DataFrame = None, y_val: pd.Series = None) -> Dict:
        """Evaluate model performance."""
        train_pred = model.predict(X_train)
        train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else train_pred
        
        metrics = {
            'train_accuracy': accuracy_score(y_train, train_pred),
            'train_roc_auc': roc_auc_score(y_train, train_proba) if len(np.unique(train_proba)) > 1 else 0.5
        }
        
        if X_val is not None and y_val is not None:
            y_val_binary = (y_val <= 3).astype(int)
            val_pred = model.predict(X_val)
            val_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else val_pred
            metrics['val_accuracy'] = accuracy_score(y_val_binary, val_pred)
            metrics['val_roc_auc'] = roc_auc_score(y_val_binary, val_proba) if len(np.unique(val_proba)) > 1 else 0.5
        
        return metrics
    
    def evaluate_on_2025_winners(self, model, X_test: pd.DataFrame, y_test: pd.Series,
                                 horse_numbers: List[int] = [14, 20, 7, 21]) -> Dict:
        """Custom evaluation metric specifically for 2025 winners pattern."""
        y_test_binary = (y_test <= 3).astype(int)
        X_test_prep = self.prepare_features(X_test, fit=False)
        
        if hasattr(model, 'predict_proba'):
            probs = model.predict_proba(X_test_prep)[:, 1]
        else:
            decision_scores = model.decision_function(X_test_prep)
            probs = 1 / (1 + np.exp(-decision_scores))
        
        # Create results DataFrame
        results_df = pd.DataFrame({
            'horse_number': X_test['horse_number'].values if 'horse_number' in X_test.columns else range(len(X_test)),
            'probability': probs
        })
        results_df = results_df.sort_values('probability', ascending=False).reset_index(drop=True)
        
        # Check rankings of target horses
        rankings = {}
        for horse_num in horse_numbers:
            if horse_num in results_df['horse_number'].values:
                rank = results_df[results_df['horse_number'] == horse_num].index[0] + 1
                prob = results_df[results_df['horse_number'] == horse_num]['probability'].iloc[0]
                rankings[horse_num] = {'rank': rank, 'probability': prob}
        
        # Calculate scores
        score = 0
        if 14 in rankings:  # Winner should be #1
            if rankings[14]['rank'] == 1:
                score += 10
            elif rankings[14]['rank'] <= 3:
                score += 5
            elif rankings[14]['rank'] <= 5:
                score += 2
        
        # Top 3 finishers should be in top 10
        top3_horses = [14, 20, 7]
        in_top10 = sum(1 for h in top3_horses if h in rankings and rankings[h]['rank'] <= 10)
        score += in_top10 * 3
        
        # All 4 should be in top 15
        in_top15 = sum(1 for h in horse_numbers if h in rankings and rankings[h]['rank'] <= 15)
        score += in_top15
        
        return {
            'custom_score': score,
            'rankings': rankings,
            'top3_in_top10': in_top10,
            'all_in_top15': in_top15
        }
    
    def train_all_models(self, X_train: pd.DataFrame, y_train: pd.Series,
                         X_val: pd.DataFrame = None, y_val: pd.Series = None,
                         X_test: pd.DataFrame = None, y_test: pd.Series = None) -> Dict:
        """Train all models and compare performance."""
        print("="*70)
        print("TRAINING MULTIPLE ML MODELS")
        print("="*70)
        
        # Prepare features
        X_train_prep = self.prepare_features(X_train, fit=True)
        if X_val is not None:
            X_val_prep = self.prepare_features(X_val, fit=False)
        else:
            X_val_prep = None
        
        results = {}
        
        # Train all models
        results['logistic_regression'] = self.train_logistic_regression(
            X_train_prep, y_train, X_val_prep, y_val
        )
        
        results['random_forest'] = self.train_random_forest(
            X_train_prep, y_train, X_val_prep, y_val
        )
        
        results['gradient_boosting'] = self.train_gradient_boosting(
            X_train_prep, y_train, X_val_prep, y_val
        )
        
        results['ensemble'] = self.train_ensemble(
            X_train_prep, y_train, X_val_prep, y_val
        )
        
        # Find best model
        best_score = 0
        for name, metrics in results.items():
            score = metrics.get('val_roc_auc', metrics.get('train_roc_auc', 0))
            if score > best_score:
                best_score = score
                self.best_model_name = name
                self.best_model = self.models[name]
        
        # Evaluate on test set if provided (for 2025 winners pattern)
        if X_test is not None and y_test is not None:
            print("\n" + "="*70)
            print("EVALUATING ON 2025 WINNERS PATTERN")
            print("="*70)
            
            for name, model in self.models.items():
                eval_results = self.evaluate_on_2025_winners(model, X_test, y_test)
                results[name]['2025_eval'] = eval_results
                print(f"\n{name.upper()}:")
                print(f"  Custom Score: {eval_results['custom_score']}")
                print(f"  Top 3 in Top 10: {eval_results['top3_in_top10']}/3")
                print(f"  All in Top 15: {eval_results['all_in_top15']}/4")
                if eval_results['rankings']:
                    print(f"  Rankings:")
                    for horse_num, info in eval_results['rankings'].items():
                        print(f"    Horse {horse_num}: Rank {info['rank']}, Prob: {info['probability']:.4f}")
            
            # Re-evaluate best model based on custom score
            best_custom_score = 0
            for name, metrics in results.items():
                if '2025_eval' in metrics:
                    score = metrics['2025_eval']['custom_score']
                    if score > best_custom_score:
                        best_custom_score = score
                        self.best_model_name = name
                        self.best_model = self.models[name]
        
        print("\n" + "="*70)
        print("MODEL COMPARISON")
        print("="*70)
        for name, metrics in results.items():
            print(f"\n{name.upper()}:")
            print(f"  Train Accuracy: {metrics['train_accuracy']:.4f}")
            print(f"  Train ROC-AUC: {metrics['train_roc_auc']:.4f}")
            if 'val_accuracy' in metrics:
                print(f"  Val Accuracy: {metrics['val_accuracy']:.4f}")
                print(f"  Val ROC-AUC: {metrics['val_roc_auc']:.4f}")
        
        print(f"\nðŸ† BEST MODEL: {self.best_model_name.upper()}")
        print("="*70)
        
        return results
    
    def predict_proba_top3(self, X: pd.DataFrame) -> np.ndarray:
        """Predict top-3 probability using best model."""
        if self.best_model is None:
            raise ValueError("No model trained. Call train_all_models first.")
        
        X_prep = self.prepare_features(X, fit=False)
        
        if hasattr(self.best_model, 'predict_proba'):
            return self.best_model.predict_proba(X_prep)[:, 1]
        else:
            return self.best_model.decision_function(X_prep)
    
    def save_best_model(self, filename: str = "best_model.pkl"):
        """Save the best model."""
        if self.best_model is None:
            raise ValueError("No model trained.")
        
        os.makedirs(self.model_dir, exist_ok=True)
        model_path = os.path.join(self.model_dir, filename)
        joblib.dump(self.best_model, model_path)
        joblib.dump(self.scaler, os.path.join(self.model_dir, "advanced_scaler.pkl"))
        joblib.dump(self.best_model_name, os.path.join(self.model_dir, "best_model_name.pkl"))
        print(f"\nBest model ({self.best_model_name}) saved to {model_path}")
        return model_path


def train_advanced_models(data_dir: str = "data", test_size: float = 0.2,
                          use_2025_test: bool = True) -> Tuple[AdvancedModelTrainer, Dict]:
    """Train advanced models on Melbourne Cup data."""
    print("Preparing training data...")
    X, y = prepare_training_data(data_dir=data_dir)
    
    # Split data
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # Further split training data for validation
    X_train_split, X_val, y_train_split, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )
    
    # Load 2025 test data if requested
    X_2025_test = None
    y_2025_test = None
    if use_2025_test:
        try:
            import pandas as pd
            df_2025 = pd.read_csv('data/processed/2025_lineup.csv')
            preprocessor = DataPreprocessor()
            preprocessor.load_preprocessors()
            processed_2025 = preprocessor.preprocess(df_2025, fit=False)
            
            # Create dummy targets (we know the winners)
            feature_cols = preprocessor.feature_columns
            X_2025_test = processed_2025[feature_cols]
            # Mark winners as top-3 (positions 1, 2, 3)
            actual_winners = {14: 1, 20: 2, 7: 3, 21: 4}
            y_2025_test = processed_2025['horse_number'].apply(
                lambda x: actual_winners.get(int(x), 10)  # Winners get 1-4, others get 10
            )
            
            print("\nLoaded 2025 test data for evaluation")
        except Exception as e:
            print(f"Could not load 2025 test data: {e}")
            X_2025_test = None
            y_2025_test = None
    
    # Train models
    trainer = AdvancedModelTrainer()
    results = trainer.train_all_models(
        X_train_split, y_train_split, X_val, y_val,
        X_2025_test, y_2025_test
    )
    
    # Save best model
    trainer.save_best_model()
    
    return trainer, results


if __name__ == "__main__":
    trainer, results = train_advanced_models()
    print("\nTraining complete!")

